# Reweighted Wake-sleep
Gnumpy code to train a directed neural network with stochastic layers and rbm prior on the MNIST dataset.
The possible algorithms are wake-sleep and reweighted wake-sleep as described here: https://arxiv.org/abs/1406.2751.

# Code
Download datasets from http://yann.lecun.com/exdb/mnist/ and place in /datasets/ directory (can be modified in config.py).

The command python hm_rws.py runs the reweighted wakesleep algorithm with default settings. This fits the inference network q to samples generated from q, reweighted as in https://arxiv.org/abs/1406.2751

The command python hm_contrastq.py runs the wakesleep algorithm with default settings. This fits the inference network q to samples generated from the generative network p, as in the usual wake-sleep algorithm.

The rbm prior in each case is trained using contrastive divergence.

The default settings are 2 stochastic layers with 500 units each. The network is pretrained using a greedy algorithm for 20 passes on the training set and then runs the chosen algorithm for 50 passes on the training set.

The output is a visualization of digits generated by the network and the log probability it assigns to the test set, evaluated using AIS (annealed importance sampling).

# Credits
Yura Burda and Jack Klys
